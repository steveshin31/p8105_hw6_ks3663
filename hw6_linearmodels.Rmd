---
title: "hw6_Linear Models"
author: "Kee-Young Shin"
date: "November 23, 2018"
output: github_document
---

```{r}
library(tidyverse)
library(RCurl)
library(modelr)
```

## Problem 1
```{r}
# import data
homicide_df = read.csv(text = 
    getURL("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"))
```

```{r}
# clean and wrangle data 
homicide_df = homicide_df %>% 
  unite(city_state, city:state, sep = ", ") %>% 
  janitor::clean_names() %>% 
  mutate(binary_solved = ifelse(disposition == "Closed by arrest", 1, 0),
         victim_age = as.numeric(victim_age),
         victim_race = as.factor(ifelse(victim_race == "White", "white", "non-white")),
         victim_race = relevel(victim_race, ref = "white")) %>% 
  filter(!((city_state == "Phoenix, AZ"))) %>% 
  filter(!(city_state == "Dallas, TX")) %>% 
  filter(!(city_state == "Kansas City, MO")) %>% 
  filter(!(city_state == "Tulsa, AL")) %>% 
  filter(!(victim_sex == "Unknown"))
  
```


```{r}
# fit regression for Baltimore, MD
baltimore_regression = homicide_df %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(binary_solved ~ victim_age + victim_sex + victim_race, data = ., 
      family = binomial()) 

```

```{r}
# show output with confidence intervals 
baltimore_regression %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         conf_low = OR - 1.96 * std.error,
         conf_high = OR + 1.96 * std.error) %>% 
  filter(term == "victim_racenon-white") %>% 
  select(term, OR, conf_low, conf_high)

```


```{r}

# run regression on all cities 
glm_output = homicide_df %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(output = map(data, ~glm(binary_solved ~ victim_age + victim_sex + 
                                 victim_race, data = .x, family = binomial())), 
         output = map(output, broom::tidy)) %>% 
  select(-data) %>% 
  unnest() %>% 
  mutate(OR = exp(estimate),
         conf_low = OR - 1.96 * std.error,
         conf_high = OR + 1.96 * std.error) %>% 
  select(city_state, term, OR, conf_low, conf_high) %>% 
  filter(term == "victim_racenon-white")
  
  
glm_output


```
```{r}
# create plot 
glm_output %>% 
  mutate(city_state = as.factor(city_state),
         city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
    geom_point() +
    geom_errorbar(ymin = glm_output$conf_low, 
                  ymax = glm_output$conf_high) +
    theme(axis.text.x = element_text(angle = 80, hjust = 1))
```

The odds ratio for Boston, MA is the lowest while highest for Tampa, FL. In other words, the odds of solving homicide for a non-white is much higher than for whites in Tampa, FL, keeping all other variables fixed. The confidence intervals are wide around the estimates due to high standard errors. 

## Problem 2
```{r}
# load and clean data 
birthweight_df = read.csv("./birthweight.csv") %>% 
  janitor::clean_names() 
```
```{r}
library(leaps)

# propose regression model
full_model = lm(bwt ~ ., data = birthweight_df)
step_model = step(full_model, direction = "both")
step_model
```

To select a model, a stepwise regression was performed. This stepwise selection process, which is a combination of forward and backward selections, returns a model that lowers the prediction error. The model reduced the full model containing 19 predictors, to a model containing 13 predictors. 

```{r}
# plot residuals for fitted values
birthweight_df %>% 
  add_predictions(step_model) %>% 
  add_residuals(step_model) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point()
```
```{r}
# second model
second_linear_model = lm(bwt ~ blength + gaweeks, data = birthweight_df)

# third model 
third_linear_model = lm(bwt ~ bhead + blength + babysex + bhead * blength + 
                          bhead * babysex + blength * babysex + 
                          bhead * blength * babysex, data = birthweight_df)

```

```{r}
# cross validation
cv_df = 
  crossv_mc(birthweight_df, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>% 
  mutate(step_model = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome
                                     + gaweeks + menarche + mheight + momage + mrace +
                                     parity + ppwt + smoken, data = .x)),
         second_model = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         third_model = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * 
                                      blength + bhead * babysex + blength * babysex + 
                                      bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse_first = map2_dbl(step_model, test, ~rmse(model = .x, data = .y)),
         rmse_second = map2_dbl(second_model, test, ~rmse(model = .x, data = .y)),
         rmse_third = map2_dbl(third_model, test, ~rmse(model = .x, data = .y)))
  

```
```{r}
# compare models
cv_df %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>% 
  mutate(model = str_replace(model, "rmse_", ""),
         model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()

```

Looking at the rmse plot, we can see that the second model is clearly the least effective in predicting the outcome. the step model and the third model (the one containing the interactions) are quite evenly matched. I would suggest picking the third model since since there are more ways to interpret its results. 
